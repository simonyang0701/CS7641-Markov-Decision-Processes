{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valueIter(env, gamma = 1.0, theta = 1e-10):\n",
    "    nStates = env.nS\n",
    "    nActions = env.nA\n",
    "    P = env.P\n",
    "    v = np.zeros(nStates)\n",
    "    iters = 0\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        v_old = np.copy(v)\n",
    "        Q = np.zeros((nStates, nActions))\n",
    "        for s in range(nStates):\n",
    "            for a in range(nActions):\n",
    "                for prob, s_next, reward, done in P[s][a]:\n",
    "                    if not done:\n",
    "                        Q[s][a] += prob * (reward + (gamma* v_old[s_next]))\n",
    "                    else:\n",
    "                        Q[s][a] += prob * (reward)\n",
    "        v = np.max(Q,1)\n",
    "        if(np.max(np.abs(v - v_old)) < theta):\n",
    "            converged = True\n",
    "        iters += 1\n",
    "    return v, iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPolicy(env,v, gamma = 1.0):\n",
    "    nStates = env.nS\n",
    "    nActions = env.nA\n",
    "    P = env.P\n",
    "    policy = np.zeros(nStates)\n",
    "    for s in range(nStates):\n",
    "        action_values = np.zeros(nActions)\n",
    "        for a in range(nActions):\n",
    "            for prob, s_next, r, done in P[s][a]:\n",
    "                if not done:\n",
    "                    action_values[a] += (prob * (r + gamma * v[s_next]))\n",
    "                else:\n",
    "                    action_values[a] += (prob * (r))\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    return policy\n",
    "\n",
    "def runPolicy(env, policy, useDiscount = True, gamma = 1.0):\n",
    "    reward = 0\n",
    "    s = env.reset()\n",
    "    i = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        s, r, done, _ = env.step(policy[s])\n",
    "        if useDiscount:\n",
    "            reward += (gamma**i * r)\n",
    "        else:\n",
    "            reward += r\n",
    "        i += 1\n",
    "    return reward\n",
    "\n",
    "def scorePolicy(env, policy, useDiscount = True, gamma = 1.0,  nSamples = 100):\n",
    "    return np.mean([runPolicy(env, policy, useDiscount, gamma) for _ in range(nSamples)])\n",
    "\n",
    "def getPolicyV(env, policy, gamma = 1.0, theta = 1e-10, maxIters = 2000):\n",
    "    nStates = env.nS\n",
    "    nActions = env.nA\n",
    "    P = env.P\n",
    "    v = np.zeros(nStates)\n",
    "    iters = 0\n",
    "    converged = False\n",
    "    while not converged and iters < maxIters:\n",
    "        v_prev = np.copy(v)\n",
    "        for s in range(nStates):\n",
    "            a = policy[s]\n",
    "            action_value = 0\n",
    "            for prob, s_next, r, done in P[s][a]:\n",
    "                if not done:\n",
    "                    action_value += prob * (r + gamma * v_prev[s_next])\n",
    "                else:\n",
    "                    action_value += prob * (r)\n",
    "            v[s] = action_value\n",
    "        if(np.max(np.abs(v - v_prev)) < theta):\n",
    "            converged = True\n",
    "        iters += 1\n",
    "    return v\n",
    "\n",
    "\n",
    "def policyIter(env, gamma = 1.0, maxIters = 200000,  theta = 1e-10, nSamples = 100):\n",
    "    nStates = env.nS\n",
    "    nActions = env.nA\n",
    "    P = env.P\n",
    "    converged = False\n",
    "    policy = np.random.choice(nActions, nStates)\n",
    "    i = 0\n",
    "    while i < maxIters and not converged:\n",
    "        policy_prev = np.copy(policy)\n",
    "        v = getPolicyV(env, policy, gamma, theta)\n",
    "        policy = getPolicy(env, v, gamma)\n",
    "\n",
    "        if(np.all(policy == policy_prev)):\n",
    "            converged = True\n",
    "        i += 1\n",
    "    return policy, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(problem, maxIters = 200, gamma = .99, alpha = .1, episodes = 500000, epsilon = 1, epsilon_min = .2, epsilon_decay = .999):\n",
    "    nState = problem.observation_space.n\n",
    "    nAction = problem.action_space.n\n",
    "    #Q = np.zeros((nState, nAction))\n",
    "    Q = np.random.rand(nState, nAction)\n",
    "    cnt = Counter()\n",
    "    for i in range(episodes):\n",
    "        s = problem.reset()\n",
    "        done = False\n",
    "        Q_old = Q.copy()\n",
    "        step = 0\n",
    "        while not done:\n",
    "            a = problem.action_space.sample()\n",
    "            if random.uniform(0,1) > epsilon:            \n",
    "                a = np.argmax(Q[s])\n",
    "            s_, r, done, info = problem.step(a)\n",
    "            q_old = Q[s,a]\n",
    "            q_new = (1-alpha)*q_old + alpha*(r+gamma*np.max(Q[s_,:]))\n",
    "            Q[s,a] = q_new\n",
    "            s = s_\n",
    "            step += 1\n",
    "        epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "        pol = np.argmax(Q, 1)\n",
    "        cnt[str(pol)] += 1\n",
    "        # if(i%100 == 99):\n",
    "        #     most_common = cnt.most_common(3)\n",
    "        #     spol = reduce(lambda x,y: str(x)+str(y), pol)\n",
    "        #     score = scorePolicy(problem, pol)\n",
    "        #     print(\"iteration: {}\\t epsilon: {}\\t policy: {}\\t score: {} \\t most common: {}\".format(i,epsilon,spol,score,[x[1] for x in most_common]))\n",
    "        \n",
    "    return np.argmax(Q, 1), episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_lake1 = gym.make(\"FrozenLake-v0\")\n",
    "frozen_lake1_env = frozen_lake1.env\n",
    "frozen_lake2 = gym.make(\"FrozenLake8x8-v0\")\n",
    "frozen_lake2_env = frozen_lake2.env\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "taxi_env = taxi.env\n",
    "\n",
    "problems = [frozen_lake1, frozen_lake2, taxi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- <FrozenLakeEnv<FrozenLake-v0>> -----\n",
      "states: 16\n",
      "actions: 4\n",
      "Value Iteration:\n",
      "\ttime: 0.18707549999999173\n",
      "\titers: 806\n",
      "\tscore: 0.88\n",
      "Policy Iteration:\n",
      "\ttime: 0.09995000000003529\n",
      "\titers: 4\n",
      "\tscore: 0.85\n",
      "Same Policy: True\n",
      "Q-learning:\n",
      "\ttime: 356.81261530000006\n",
      "\titers: 500000\n",
      "\tscore: 0.76\n",
      "\n",
      "Sample of policies:\tValue Iteration\tPolicy Iteration\tQ-Learning policy\n",
      "[0. 3. 3. 3. 0. 0. 0. 0.] [0. 3. 3. 3. 0. 0. 0. 0.] [0 3 0 3 0 0 0 2]\n",
      "----- <FrozenLakeEnv<FrozenLake8x8-v0>> -----\n",
      "states: 64\n",
      "actions: 4\n",
      "Value Iteration:\n",
      "\ttime: 1.2107356999999865\n",
      "\titers: 1425\n",
      "\tscore: 1.0\n",
      "Policy Iteration:\n",
      "\ttime: 2.667106800000056\n",
      "\titers: 12\n",
      "\tscore: 1.0\n",
      "Same Policy: True\n",
      "Q-learning:\n",
      "\ttime: 538.6307535000001\n",
      "\titers: 500000\n",
      "\tscore: 0.15\n",
      "\n",
      "Sample of policies:\tValue Iteration\tPolicy Iteration\tQ-Learning policy\n",
      "[3. 2. 2. 2. 2. 2. 2. 2.] [3. 2. 2. 2. 2. 2. 2. 2.] [1 2 2 2 2 2 2 1]\n",
      "----- <TaxiEnv<Taxi-v3>> -----\n",
      "states: 500\n",
      "actions: 6\n",
      "Value Iteration:\n",
      "\ttime: 0.09314059999996971\n",
      "\titers: 19\n",
      "\tscore: 7.8\n",
      "Policy Iteration:\n",
      "\ttime: 28.794077600000037\n",
      "\titers: 17\n",
      "\tscore: 8.28\n",
      "Same Policy: True\n",
      "Q-learning:\n",
      "\ttime: 790.9325297\n",
      "\titers: 500000\n",
      "\tscore: 7.58\n",
      "\n",
      "Sample of policies:\tValue Iteration\tPolicy Iteration\tQ-Learning policy\n",
      "[4. 4. 4. 4. 0. 0. 0. 0.] [4. 4. 4. 4. 0. 0. 0. 0.] [2 4 4 4 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for problem in problems:\n",
    "    env = problem.env\n",
    "    print(\"-----\" , str(env), \"-----\")\n",
    "    print(\"states: {}\\nactions: {}\".format(env.nS, env.nA))\n",
    "    #####\n",
    "    # do value iteration\n",
    "    t0 = time.perf_counter()\n",
    "    v_value, i_value = valueIter(env, 1.0)\n",
    "    tf = time.perf_counter()\n",
    "    t_value = tf-t0\n",
    "    # get policy\n",
    "    p_star_value = getPolicy(env, v_value)\n",
    "    score_value = scorePolicy(env, p_star_value, False)\n",
    "    #####\n",
    "    print(\"Value Iteration:\\n\\ttime: {}\\n\\titers: {}\\n\\tscore: {}\".format(t_value, i_value,score_value))\n",
    "    ############\n",
    "\n",
    "    ############\n",
    "    # do policy iteration\n",
    "    t0 = time.perf_counter()\n",
    "    p_star_policy, i_policy = policyIter(env)\n",
    "    tf = time.perf_counter()\n",
    "    t_policy = tf-t0\n",
    "    # derive value matrix\n",
    "    v_policy = getPolicyV(env, p_star_policy)\n",
    "    score_policy = scorePolicy(env, p_star_policy, False)\n",
    "    print(\"Policy Iteration:\\n\\ttime: {}\\n\\titers: {}\\n\\tscore: {}\".format(t_policy, i_policy,score_policy))\n",
    "    #####    \n",
    "    #print(v_policy, \"\\n\", p_star_policy)\n",
    "    print(\"Same Policy: {}\".format(np.all(p_star_value == p_star_policy)))\n",
    "    ############\n",
    "    ############\n",
    "    # do Q-learning\n",
    "    t0 = time.perf_counter()\n",
    "    p_star_Q, i_Q = QLearning(problem)\n",
    "    tf = time.perf_counter()\n",
    "    t_Q = tf-t0\n",
    "    score_Q = scorePolicy(env, p_star_Q, False)\n",
    "    print(\"Q-learning:\\n\\ttime: {}\\n\\titers: {}\\n\\tscore: {}\".format(t_Q, i_Q,score_Q))\n",
    "    print(\"\")\n",
    "    print(\"Sample of policies:\\tValue Iteration\\tPolicy Iteration\\tQ-Learning policy\")\n",
    "    if(len(p_star_Q) > 8):\n",
    "        print(p_star_value[:8], p_star_policy[:8], p_star_Q[:8])\n",
    "    else:\n",
    "        print(p_star_value, p_star_policy, p_star_Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
